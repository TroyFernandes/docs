{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>I've been self-hosting &amp; homelabbing since 2016 and I often times find myself in this scenario:</p> <p><pre><code>&gt; Find something interesting on Github/Reddit\n&gt; Learn how to set it up in a test environment\n&gt; Move on &amp; forget about it\n&gt; Months later it shows up again\n&gt; I have some hardware that I can run this on permanently\n&gt; Forget how I set it up originally &amp; can't find the guides again\n&gt; ...\n</code></pre> </p> <p>I always prefer text guides over any other medium + it's always a bonus when they're short. </p> <p>So here's my version of that; very informal docs on things I've setup, to refer back to.</p> <p>Maybe you'll find it useful. </p>"},{"location":"arch_linux/recover_systemd-boot/","title":"Recover systemd-boot","text":"<p>Sometimes when updating your system (in my case using <code>yay</code>), when you go to reboot your boot manager you will only have the option <code>Reboot into firmware interface</code>.</p> <p>The issue can be anything really, but usually it happens when installing a package, and when an update comes around, something breaks.</p> <p>This guide was written with <code>EndeavourOS</code> in mind.</p>"},{"location":"arch_linux/recover_systemd-boot/#resources","title":"Resources","text":"<ul> <li>chroot</li> <li>arch-chroot</li> <li>Help recovering Systemd-boot</li> </ul>"},{"location":"arch_linux/recover_systemd-boot/#how-to","title":"How-to","text":"<ol> <li>Boot into the live environment; I used EndeavourOS.</li> <li>Change to root. <code>sudo su</code></li> <li>List the system partitions. <code>lsblk -f</code>. You should see something like this. The relevant section is <code>nvme0n1</code>, more specifically <code>nvme0n1p1</code> and <code>nvme0n1p2</code>. We need to mount both of these. <pre><code>[root@EndeavourOS liveuser]# lsblk -f\nNAME        FSTYPE   FSVER            LABEL       UUID                                 FSAVAIL FSUSE% MOUNTPOINTS\nloop0       squashfs 4.0                                                                     0   100% /run/archiso/airootfs\nsda\n\u251c\u2500sda1      exfat    1.0              Ventoy      4E21-0000\n\u2502 \u2514\u2500ventoy  iso9660  Joliet Extension EOS_202406  2024-06-25-11-30-26-00                     0   100% /run/archiso/bootmnt\n\u2514\u2500sda2      vfat     FAT16            VTOYEFI     223C-F3F8\nnvme0n1\n\u251c\u2500nvme0n1p1 vfat     FAT32                        5E74-E696\n\u251c\u2500nvme0n1p2 ext4     1.0              endeavouros 1a236633-49a9-4fa6-918a-8129dd30617a   33.5G    79% /run/media/liveuser/endeavouros\n\u2514\u2500nvme0n1p3 swap     1                swap        3556ab04-2191-4da9-b168-3df725fdaaab\n</code></pre></li> <li>Mount the root partition. <code>mount /dev/nvme0n1p2 /mnt</code></li> <li>Mount the efi partition. <code>mount /dev/nvme0n1p1 /mnt/efi</code></li> <li>arch-chroot into the mounted system. <code>arch-chroot /mnt</code></li> <li>Optional step. I need to uninstall the package which broke my system which in this case is <code>zfs-utils</code>. <code>pacman -Rns zfs-utils</code>. This removes the package as well as the dependencies.</li> <li>Now the actual fix. Reinstalling the kernel. Run <code>reinstall-kernels</code> The ouput should show no errors and look something like this: <pre><code>[root@EndeavourOS /]# reinstall-kernels\nInstalling kernel 6.10.8-arch1-1\ndracut[I]: Executing: /usr/bin/dracut --no-hostonly --force /efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd-fallback 6.10.8-arch1-1\ndracut[I]: *** Including module: systemd ***\ndracut[I]: *** Including module: systemd-ask-password ***\ndracut[I]: *** Including module: systemd-initrd ***\ndracut[I]: *** Including module: systemd-journald ***\ndracut[I]: *** Including module: systemd-modules-load ***\ndracut[I]: *** Including module: systemd-pcrphase ***\ndracut[I]: *** Including module: systemd-sysctl ***\ndracut[I]: *** Including module: systemd-sysusers ***\ndracut[I]: *** Including module: systemd-tmpfiles ***\ndracut[I]: *** Including module: systemd-udevd ***\ndracut[I]: *** Including module: modsign ***\ndracut[I]: *** Including module: i18n ***\ndracut[I]: *** Including module: btrfs ***\ndracut[I]: *** Including module: crypt ***\ndracut[I]: *** Including module: dm ***\ndracut[I]: *** Including module: kernel-modules ***\ndracut[I]: *** Including module: kernel-modules-extra ***\ndracut[I]: *** Including module: lvm ***\ndracut[I]: *** Including module: mdraid ***\ndracut[I]: *** Including module: nvdimm ***\ndracut[I]: *** Including module: pcmcia ***\ndracut[I]: *** Including module: qemu ***\ndracut[I]: *** Including module: qemu-net ***\ndracut[I]: *** Including module: systemd-cryptsetup ***\ndracut[I]: *** Including module: fido2 ***\ndracut[I]: *** Including module: pkcs11 ***\ndracut[I]: *** Including module: hwdb ***\ndracut[I]: *** Including module: lunmask ***\ndracut[I]: *** Including module: resume ***\ndracut[I]: *** Including module: rootfs-block ***\ndracut[I]: *** Including module: terminfo ***\ndracut[I]: *** Including module: udev-rules ***\ndracut[I]: *** Including module: virtiofs ***\ndracut[I]: *** Including module: dracut-systemd ***\ndracut[I]: *** Including module: usrmount ***\ndracut[I]: *** Including module: base ***\ndracut[I]: *** Including module: fs-lib ***\ndracut[I]: *** Including module: shutdown ***\ndracut[I]: *** Including modules done ***\ndracut[I]: *** Installing kernel module dependencies ***\ndracut[I]: *** Installing kernel module dependencies done ***\ndracut[I]: *** Resolving executable dependencies ***\ndracut[I]: *** Resolving executable dependencies done ***\ndracut[I]: *** Hardlinking files ***\ndracut[I]: *** Hardlinking files done ***\ndracut[I]: *** Generating early-microcode cpio image ***\ndracut[I]: *** Constructing GenuineIntel.bin ***\ndracut[I]: *** Store current command line parameters ***\ndracut[I]: *** Stripping files ***\ndracut[I]: *** Stripping files done ***\ndracut[I]: *** Creating image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd-fallback' ***\ndracut[I]: *** Creating initramfs image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd-fallback' done ***\ndracut[I]: Executing: /usr/bin/dracut --hostonly --no-hostonly-cmdline -f /efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd 6.10.8-arch1-1\ndracut[I]: *** Including module: systemd ***\ndracut[I]: *** Including module: systemd-ask-password ***\ndracut[I]: *** Including module: systemd-initrd ***\ndracut[I]: *** Including module: systemd-journald ***\ndracut[I]: *** Including module: systemd-modules-load ***\ndracut[I]: *** Including module: systemd-sysctl ***\ndracut[I]: *** Including module: systemd-sysusers ***\ndracut[I]: *** Including module: systemd-tmpfiles ***\ndracut[I]: *** Including module: systemd-udevd ***\ndracut[I]: *** Including module: i18n ***\ndracut[I]: *** Including module: kernel-modules ***\ndracut[I]: *** Including module: kernel-modules-extra ***\ndracut[I]: *** Including module: pcmcia ***\ndracut[I]: *** Including module: hwdb ***\ndracut[I]: *** Including module: resume ***\ndracut[I]: *** Including module: rootfs-block ***\ndracut[I]: *** Including module: terminfo ***\ndracut[I]: *** Including module: udev-rules ***\ndracut[I]: *** Including module: dracut-systemd ***\ndracut[I]: *** Including module: usrmount ***\ndracut[I]: *** Including module: base ***\ndracut[I]: *** Including module: fs-lib ***\ndracut[I]: *** Including module: shutdown ***\ndracut[I]: *** Including modules done ***\ndracut[I]: *** Installing kernel module dependencies ***\ndracut[I]: *** Installing kernel module dependencies done ***\ndracut[I]: *** Resolving executable dependencies ***\ndracut[I]: *** Resolving executable dependencies done ***\ndracut[I]: *** Hardlinking files ***\ndracut[I]: *** Hardlinking files done ***\ndracut[I]: *** Generating early-microcode cpio image ***\ndracut[I]: *** Constructing GenuineIntel.bin ***\ndracut[I]: *** Store current command line parameters ***\ndracut[I]: *** Stripping files ***\ndracut[I]: *** Stripping files done ***\ndracut[I]: *** Creating image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd' ***\ndracut[I]: *** Creating initramfs image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd' done ***\n[root@EndeavourOS /]#\n</code></pre></li> </ol> <p>Reboot and you should see your regular boot manager options.</p>"},{"location":"docker/cheatsheet/","title":"Cheatsheet","text":""},{"location":"docker/cheatsheet/#rebuilding-a-dockerfile-image","title":"Rebuilding a Dockerfile &amp; Image","text":"<pre><code>docker compose build --no-cache\ndocker compose up -d\n</code></pre>"},{"location":"docker/cheatsheet/#view-live-docker-logs","title":"View Live docker logs","text":"<pre><code>docker logs --follow &lt;ID&gt;\n</code></pre>"},{"location":"docker/docker_context/","title":"Docker Context","text":"<p>How to manage another docker instance e.g. deploy/manage docker compose files on a Raspberry Pi</p>"},{"location":"docker/docker_context/#resources","title":"Resources","text":"<ul> <li>Docker Context</li> </ul>"},{"location":"docker/docker_context/#prerequisites","title":"Prerequisites","text":"<p>You should set up passwordless ssh access between your machines.</p>"},{"location":"docker/docker_context/#adding-a-remote-context","title":"Adding a Remote Context","text":"<ol> <li> <p>Create the context: <code>docker context create dietpi1 --docker \"host=ssh://root@192.168.1.11\"</code></p> </li> <li> <p>View the new context using: <code>docker context ls</code> <pre><code>/c/Windows/System32 \u276f docker context ls\nNAME              DESCRIPTION                               DOCKER ENDPOINT                             ERROR\ndefault           Current DOCKER_HOST based configuration   npipe:////./pipe/docker_engine\ndesktop-linux *   Docker Desktop                            npipe:////./pipe/dockerDesktopLinuxEngine\ndietpi1                                                     ssh://root@192.168.1.11\n</code></pre></p> </li> <li> <p>If you need to update the context: <code>docker context update --description \"RaspberryPi 3b; DietPi\" --docker \"host=ssh://root@192.168.1.11\" dietpi1</code></p> </li> </ol>"},{"location":"docker/docker_context/#deploying-a-container-to-the-remote","title":"Deploying a Container to the Remote","text":"<p>Simply create your docker compose file and run: <code>docker-compose --context dietpi1 up -d</code></p>"},{"location":"docker/healthchecks.io/","title":"Healthchecks.io","text":"<p>Setting up a Healthchecks container with e-mail notifications using Gmail.</p>"},{"location":"docker/healthchecks.io/#resources","title":"Resources","text":"<ul> <li>Healthchecks Docs</li> <li>Container Image</li> <li>Gmail App Passwords</li> </ul>"},{"location":"docker/healthchecks.io/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need an app password created for your gmail account you want to send emails from.</li> </ul>"},{"location":"docker/healthchecks.io/#setup","title":"Setup","text":"<ol> <li>Grab the docker-compose file from the container page. I'll show mine:</li> </ol> <p><pre><code>version: \"3\"\nservices:\n  healthchecks:\n    image: healthchecks/healthchecks:latest\n    container_name: healthchecks\n    environment:\n      - ALLOWED_HOSTS=*\n      - DB=sqlite\n      - DB_NAME=/data/hc.sqlite\n      - DEBUG=True\n      - DEFAULT_FROM_EMAIL=email@gmail.com\n      - EMAIL_HOST=smtp.gmail.com\n      - EMAIL_HOST_PASSWORD=qqqqwwwweeeerrrr\n      - EMAIL_HOST_USER=email@gmail.com\n      - EMAIL_PORT=587\n      - EMAIL_USE_TLS=True\n      - SECRET_KEY=---\n      - SITE_ROOT=http://192.168.1.11:8000\n    ports:\n      - 8000:8000\n    volumes:\n      - healthchecks-data:/data\n    restart: unless-stopped\nvolumes:\n    healthchecks-data:\n</code></pre> NOTE: When you create a gmail app password, it creates a 16 character password, and shows it to you like this: <code>qqqq wwww eeee rrrr</code>. You need to enter it like above with no spaces, otherwise it won't work and you'll get the error: <code>Server returned an error: 535-5.7.8 Username and Password not accepted</code></p> <ol> <li>Pull the image and once it's created, create the super-user account by running: </li> </ol> <p><code>docker-compose exec healthchecks /opt/healthchecks/manage.py createsuperuser</code></p>"},{"location":"docker/healthchecks.io/#creating-an-example-check","title":"Creating an Example Check","text":"<ol> <li> <p>There should be an example check created when you first setup healthchecks. You can also test out the email notifications under <code>Integrations</code>.</p> <p>What I find useful about healthchecks is that you can signal the start and end of a task you want to be monitored. My use-case is to monitor cron jobs; Let's see an example.</p> </li> <li> <p>We'll signal a start by appending <code>/start</code> to the url. To signal an end, simply omit the <code>/start</code> from the url. Here's an example cron job that rsyncs a directory from one drive to another.</p> </li> </ol> <pre><code>#!/bin/bash\ncurl -fsS -m 10 --retry 5 -o /dev/null http://192.168.1.11:8000/ping/f77a6fc5-ca6c-49fe-bfcd-9e5f2da335dd/start\n\nrsync --delete-after -avP /mnt/cache_ssd/Music/ /mnt/disk4/Music/\n\ncurl -fsS -m 10 --retry 5 -o /dev/null http://192.168.1.11:8000/ping/f77a6fc5-ca6c-49fe-bfcd-9e5f2da335dd\n</code></pre> <p>You'll need to configure the check with a proper grace period to get alerting. The healthchecks docs are easy to follow and show you how.</p>"},{"location":"homelab/network_diagram/","title":"Network diagram","text":""},{"location":"homelab/rack/","title":"Rack","text":""},{"location":"kubernetes/cheatsheet/","title":"Cheatsheet","text":""},{"location":"kubernetes/cheatsheet/#get-all-running-namespaces","title":"Get all running namespaces","text":"<p><code>kubectl get svc --all-namespaces -o wide</code></p>"},{"location":"kubernetes/cheatsheet/#create-a-deployment","title":"Create a deployment","text":"<p><code>kubectl create -f [filename.yml]</code></p>"},{"location":"kubernetes/cheatsheet/#delete-a-deployment","title":"Delete a deployment","text":"<p><code>kubectl delete -f [filename.yml]</code></p>"},{"location":"kubernetes/cheatsheet/#stop-pod-by-namespace","title":"Stop pod by namespace","text":"<p><code>kubectl delete --all pods --namespace=[namespace]</code></p>"},{"location":"kubernetes/cheatsheet/#stop-deployment-by-namespace","title":"Stop deployment by namespace","text":"<p><code>kubectl delete --all deployments --namespace=firefox</code></p>"},{"location":"kubernetes/cheatsheet/#switch-to-namespace","title":"Switch to namespace","text":"<p><code>kubectl config set-context --current --namespace=my-namespace</code></p>"},{"location":"kubernetes/cheatsheet/#edit-deployment","title":"Edit Deployment","text":"<p><code>kubectl edit deployment/my-nginx</code></p>"},{"location":"kubernetes/cheatsheet/#restart-deployment","title":"Restart Deployment","text":"<p><code>kubectl rollout restart deployment &lt;deployment_name&gt; -n &lt;namespace&gt;</code></p>"},{"location":"kubernetes/cheatsheet/#delete-service","title":"Delete Service","text":"<p><code>kubectl delete service kubernetes-dashboard -n kube-system</code></p>"},{"location":"kubernetes/cheatsheet/#delete-namespace","title":"Delete namespace","text":"<p><code>kubectl delete ns cattle-system</code></p>"},{"location":"opnsense/caddy/","title":"Caddy Reverse Proxy with Lets Encrypt","text":"<p>It's time to finally get rid of those browser warnings when hosting services using <code>HTTP</code>. We'll setup caddy as an internal reverse proxy and use DNS-01 Challenges to get a valid certificates from Let's Encrypt. </p> <p>Nothing needs to, or will be exposed from outside our network to achieve this.</p> <p>We'll use our domain providers' <code>(Porkbun)</code> to API generate and grab the certs.</p> <p>This will also enable us to reach our services with friendly names. Instead of going to Gitea with <code>192.168.1.10:3000</code>, we'll do something like <code>gitea.example.ca</code></p>"},{"location":"opnsense/caddy/#resources","title":"Resources","text":"<ul> <li>Caddy</li> <li>OPNsense Caddy Guide</li> <li>Porkbun</li> <li>Home Network Guy Unbound DNS Overrides</li> </ul>"},{"location":"opnsense/caddy/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>AdguardHome working properly with Unbound (Private reverse dns resolvers)</li> <li> <p>A domain you own; I have one registered with Porkbun. We'll assume I bought the domain <code>example.ca</code></p> </li> <li> <p>An API Key &amp; Secret Key generated for your domain through Porkbun.</p> </li> </ul>"},{"location":"opnsense/caddy/#what-is-what","title":"What is What","text":"<p>I use OPNSense's default DNS resolver <code>Unbound</code> in conjunction with <code>AdguardHome</code> for ad-blocking</p> <ul> <li>Unbound is running on OPNsense with the ip: <code>192.168.1.1:5353</code></li> <li>AdguardHome is running on OPNsense with the ip: <code>192.168.1.1:53</code></li> <li>Caddy will run on OPNsense with the ip: <code>192.168.1.1:80</code> and <code>192.168.1.1:443</code></li> </ul>"},{"location":"opnsense/caddy/#how-to","title":"How To","text":""},{"location":"opnsense/caddy/#unbound","title":"Unbound","text":"<p>Follow the guide from Home Network Guy to create the override.</p> <p>Make sure the IP address is the IP of OPNsense; In my case it is <code>192.168.1.1</code>.</p> <p>The only change when following the guide is to use a wildcard instead of the service name. In his guide, he entered <code>reverse-proxy</code> as the host, but we instead will do <code>*</code> as the host.</p> <p>Don't create the alias.</p> <p></p>"},{"location":"opnsense/caddy/#adguardhome","title":"AdguardHome","text":"<p>Since this is internal, we'll need to do a DNS rewrite to tell our DNS server to resolve any requests to <code>*.example.ca</code>.</p> <p>Under <code>Filters -&gt; DNS Rewrites</code> Add an entry with <code>*.example.ca</code> as the domain and <code>192.168.1.1</code> as the IP.</p> <p>This will let AdguardHome let Unbound resolve these requests.</p> <p></p>"},{"location":"opnsense/caddy/#caddy","title":"Caddy","text":"<ol> <li> <p>Start from the installation but skip creating the WAN firewall rule.</p> </li> <li> <p>Then continue following Creating a Simple Reverse Proxy but before doing Add Domain go to DNS Provider and select Porkbun as the provider.</p> <p>Enter your API Key from porkbun into the DNS API Standard Field and under DNS API Additional Field 1 add your Secret Key from porkbun.</p> </li> <li> <p>Continue following the guide to add a domain, but make sure to check the DNS-01 Challenge box when adding your domain.</p> </li> <li> <p>Stop before HTTP Handlers. We'll do something before this.</p> </li> <li> <p>Since we're using wildcards, we'll create subdomains for each of our services. Create a new Sub-Domain, and make sure the domain you created before is selected. Name the subdomain whatever you want. e.g if you use a domain called <code>*.example.ca</code> then your subdomain would be <code>gitea.example.ca</code></p> </li> </ol> <p></p> <ol> <li>Continue creating the HTTP Handler, but make sure to select the sub-domain we created in step 5.</li> </ol> <p></p> <ol> <li>That should be everything setup now. Caddy should grab the certs and you should now be able to go to <code>gitea.example.ca</code> and see your gitea instance being served with <code>https</code>.</li> </ol>"},{"location":"opnsense/network_booting/","title":"Network Booting with netboot.xyz &amp; OPNsense","text":"<p>Network booting is useful when you want to image a machine without having to carry around a USB drive. My main use case for setting this up is to eventually netboot raspberry pi's and mount their boot drives over NFS. </p>"},{"location":"opnsense/network_booting/#resources","title":"Resources","text":"<ul> <li>netboot.xyz</li> <li>General steps to setting up netbooting on OPNsense by u/Asche77</li> <li>In-depth guide on netbooting with OPNsense</li> <li>Complete netbootxyz/unifi/proxmox guide by TechnoTim</li> </ul> <p>Related</p> <ul> <li>Troubleshooting netbooting within a Proxmox VM</li> </ul>"},{"location":"opnsense/network_booting/#what-is-what","title":"What is what","text":"<ul> <li>OPNsense IP is <code>192.168.1.1</code></li> <li>TFTP server will be <code>192.168.1.1</code></li> <li>DHCP server is <code>192.168.1.1</code></li> <li>netbootxyz server is <code>192.168.1.11</code></li> </ul>"},{"location":"opnsense/network_booting/#how-to","title":"How To","text":""},{"location":"opnsense/network_booting/#netbootxyz-docker-setup","title":"netboot.xyz Docker Setup","text":"<ol> <li>Setup is pretty easy, just choose your install method. I'll use a docker compose file to install the container on a Debian LXC Container.</li> </ol> <pre><code>version: \"2.1\"\nservices:\n  netbootxyz:\n    image: ghcr.io/netbootxyz/netbootxyz\n    container_name: netbootxyz\n    volumes:\n      - ./config:/config\n      - ./assets:/assets\n    ports:\n      - 3000:3000\n      - 69:69/udp\n      - 8080:80\n    restart: unless-stopped\n</code></pre>"},{"location":"opnsense/network_booting/#tftp-server-opnsense-settings","title":"TFTP Server &amp; OPNsense Settings","text":"<ol> <li> <p>Install the TFTP server on OPNsense by going to [System] -&gt; [Plugin] -&gt; search for <code>tft</code>, and install <code>os-tftp</code></p> </li> <li> <p>The server won't start unless the <code>/usr/local/tftp</code> directory exists, so create it.</p> </li> <li> <p>Download the netboot.xyz efi files from the releases page and place them in the directory you created. You can use the fetch command in freebsd like <code>fetch https://github.com/netbootxyz/netboot.xyz/releases/download/2.0.82/netboot.xyz.efi</code></p> <p>I'll download the following:</p> <ul> <li>netboot.xyz-arm64.efi</li> <li>netboot.xyz.efi</li> <li>netboot.xyz.kpxe</li> </ul> </li> <li> <p>Add your netboot server to your DHCP server by going to [Services] -&gt; [ISC DHCPv4] -&gt; [Interface] -&gt; [Network booting].</p> </li> <li> <p>Add your netboot server under [Services] -&gt; [ISC DHCPv4] -&gt; [Interface] -&gt; [TFTP server].</p> </li> </ol> <p></p> <ol> <li>You should be able to now boot from the network</li> </ol>"},{"location":"software_dev_tools/git/cheatsheet/","title":"Cheatsheet","text":""},{"location":"software_dev_tools/git/cheatsheet/#ignore-local-changes-delete-new-files-and-pull-from-remote-my-favourite","title":"Ignore local changes, delete new files and pull from remote <sub><sub><sub>(my favourite)</sub></sub></sub>","text":"<pre><code>git reset --hard\ngit clean -fxd\ngit pull\n</code></pre>"},{"location":"software_dev_tools/gitea/gitea-actions/","title":"Setup Gitea Actions","text":"<p>Guide to setting up Gitea Actions under docker</p>"},{"location":"software_dev_tools/gitea/gitea-actions/#why","title":"Why?","text":"<ol> <li>Replicate Github Actions locally</li> <li>It's annoying to verify Github Actions by making a push to a repo. Gitea actions are supposed to be a drop in replacement for Github Actions.</li> <li>You can assign way more hardware to the runner</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-actions/#goals-at-the-end-of-the-day","title":"Goals at the end of the day","text":"<p>We should have runners setup and available to execute the actions within our repos.</p>"},{"location":"software_dev_tools/gitea/gitea-actions/#resources","title":"Resources","text":"<ul> <li>Gitea Actions</li> <li>Gitea Actions Quickstart</li> <li>Gitea Act Runner</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-actions/#requirements","title":"Requirements","text":"<ol> <li>A gitea server</li> <li>A machine with docker installed as well as docker-compose</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-actions/#how-to","title":"How-To","text":""},{"location":"software_dev_tools/gitea/gitea-actions/#installing-act-runner","title":"Installing Act Runner","text":"<ol> <li> <p>We need to get the registration token from our gitea server. Within the gitea server, go into the terminal and run: <code>gitea --config /etc/gitea/app.ini actions generate-runner-token</code>. We'll get a token looking similar to this: <code>4ALVqHi8lx8Yk34RIJCn9pyqgafFDjEMpNQxHEpi</code></p> <ul> <li>Note: You can't run the above command as root. Make sure you're logged into an account that isnt root. e.g: <code>su - gitea</code></li> </ul> </li> <li> <p>Create the <code>docker-compose.yml</code> file. The Gitea docs have a standard template, I'll show you what I used: <pre><code>version: \"3.8\"\nservices:\n  runner:\n    image: gitea/act_runner:nightly\n    environment:\n      CONFIG_FILE: /config.yaml\n      GITEA_INSTANCE_URL: \"http://192.168.1.232:3000/\"\n      GITEA_RUNNER_REGISTRATION_TOKEN: \"4ALVqHi8lx8Yk34RIJCn9pyqgafFDjEMpNQxHEpi\"\n      # GITEA_RUNNER_NAME: \"${RUNNER_NAME}\"\n      # GITEA_RUNNER_LABELS: \"${RUNNER_LABELS}\"\n    volumes:\n      - ./config/config.yaml:/config.yaml\n      - ./data:/data\n      - /var/run/docker.sock:/var/run/docker.sock\n</code></pre></p> </li> <li> <p><code>docker-compose up -d</code> and the runner should register with your gitea instance.</p> </li> <li> <p>Go to your gitea dashboard and do: <code>Settings -&gt; Actions -&gt; Runners</code> and you should see the runner you just created.</p> </li> <li> <p>The runner is setup now. Lets clone a repo and add an action.</p> </li> <li> <p>To add an action, within the root dir, create directory <code>.gitea/workflows/</code></p> </li> <li> <p>Add your action file. Name it whatever you like with the extension <code>.yml</code>. Lets use the example from Gitea.</p> </li> </ol> <pre><code>name: Gitea Actions Demo\nrun-name: ${{ gitea.actor }} is testing out Gitea Actions\non: [push]\n\njobs:\n  Explore-Gitea-Actions:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"The job was automatically triggered by a ${{ gitea.event_name }} event.\"\n      - run: echo \"This job is now running on a ${{ runner.os }} server hosted by Gitea!\"\n      - run: echo \"The name of your branch is ${{ gitea.ref }} and your repository is ${{ gitea.repository }}.\"\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - run: echo \"The ${{ gitea.repository }} repository has been cloned to the runner.\"\n      - run: echo \"The workflow is now ready to test your code on the runner.\"\n      - name: List files in the repository\n        run: |\n          ls ${{ gitea.workspace }}\n      - run: echo \"This job's status is ${{ job.status }}.\"\n</code></pre> <ol> <li>Now push the repo. You should see a little yellow dot next to your username, above the topmost folder on the Gitea dashboard. You can click into it to check the status of the action.</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-actions/#end","title":"End","text":"<p>Our Gitea server should now have runners available to execute actions within our repos.</p>"},{"location":"software_dev_tools/gitea/gitea-actions/#final-thoughts-learnings","title":"Final Thoughts / Learnings","text":"<p>My initial assumption was that gitea had the capability to execute actions out of the box, but this isn't the case and the reason makes sense.</p> <p>It's better to have \"Ephemeral\" runners. Which esentilally means they're created when needed, then destroyed after. This allows you to not worry about having a cleanup script after the action has completed, ensuring the action is always run in a repetable, clean state.</p> <p>You'll notice within the docker compose file we bind the docker sock. This allows the runner to create seperate containers that are destroyed after the action has completed.</p>"},{"location":"software_dev_tools/gitea/gitea-server/","title":"Self-Hosted Gitea Server","text":"<p>Guide to setting up a self-hosted Gitea server in an LXC containter on Proxmox</p>"},{"location":"software_dev_tools/gitea/gitea-server/#why","title":"Why?","text":"<ul> <li>Having a private git repo with a frontend for projects seperate from Github</li> <li>Run the equivalent of Github Actions but locally</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-server/#goals-at-the-end-of-the-day","title":"Goals at the end of the day","text":"<ul> <li>Have a local private Gitea Server running, accessible on the local network</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-server/#resources","title":"Resources","text":"<ul> <li>Gitea</li> <li>Proxmox Helper Scripts - Gitea</li> <li>Proxmox Helper Scripts - PostgreSQL</li> <li>PostgreSQL Post-Install</li> <li>Permission Denied fix when initial Gitea setup</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-server/#requirements","title":"Requirements","text":"<p>The Gitea LXC container doesn't have a database, so we need to set one up. We will use a PostgreSQL LXC container.</p>"},{"location":"software_dev_tools/gitea/gitea-server/#how-to","title":"How-To","text":""},{"location":"software_dev_tools/gitea/gitea-server/#installing-postgresql","title":"Installing PostgreSQL","text":"<ol> <li> <p>Run the Proxmox Helper Script; Follow the prompts from the script and install Adminer</p> </li> <li> <p>Follow the post-install guide up until creating the new database.</p> </li> <li> <p>Create a new user named <code>gitea</code></p> <ul> <li><code>CREATE USER gitea WITH PASSWORD 'your-password';</code></li> </ul> </li> <li> <p>Create a new database named <code>gitea</code></p> <ul> <li><code>CREATE DATABASE gitea;</code></li> </ul> </li> <li> <p>Grant the user <code>gitea</code> access to the database <code>gitea</code></p> <ul> <li><code>GRANT ALL ON DATABASE gitea TO gitea;</code></li> </ul> </li> <li> <p>Change the owner of the <code>gitea</code> database</p> <ul> <li><code>ALTER DATABASE gitea OWNER TO gitea;</code></li> </ul> </li> </ol>"},{"location":"software_dev_tools/gitea/gitea-server/#installing-gitea","title":"Installing Gitea","text":"<ol> <li>Run the Proxmox Helper Script</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-server/#end","title":"End","text":"<p>We should now have a running gitea server with a postgreSQL database</p>"},{"location":"zfs/snapshots/","title":"Snapshots","text":"<p>This will be a guide on how to leverage the features of ZFS and create snapshots of machines running ZFS.</p> <p>We'll then send them to another machine also running ZFS to have a backup.</p>"},{"location":"zfs/snapshots/#resources","title":"Resources","text":"<ul> <li>ZFS Snapshot Briefer</li> <li>Fast ZFS Send with Netcat</li> </ul>"},{"location":"zfs/snapshots/#requirements","title":"Requirements","text":"<ul> <li>Two machines configured with ZFS</li> <li>If you want to browse the snapshots via SMB/NFS you'll need to create the share on TrueNAS. I made one called <code>zfs_snapshots</code></li> </ul> <p>I'll be using my laptop running <code>NixOS 24.05</code> with ZFS as the machine to backup. The backup server(target) will be a machine running <code>TrueNAS SCALE Dragonfish-24.04.2</code></p>"},{"location":"zfs/snapshots/#how-to","title":"How-To","text":"<ol> <li>View your zfs pools using <code>zpool list</code></li> </ol> <pre><code>[troy@nixos:~]$ zpool list\nNAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT\nzroot   228G  5.99G   222G        -         -     0%     2%  1.00x    ONLINE  -\n</code></pre> <ol> <li>We'll create a snapshot of the zroot dataset. <code>sudo zfs snapshot -r DATASET@NAME</code>. Note the <code>-r</code> flag. This will create a snapshot of the dataset and all of its children. We'll run this initially and every subsequent snapshot you can exlude this flag.</li> </ol> <pre><code>[troy@nixos:~]$ sudo zfs snapshot -r zroot@nixos_base\n</code></pre> <ol> <li>You can view the snapshots using <code>zfs list -t snapshot</code></li> </ol> <pre><code>[troy@nixos:~]$ zfs list -t snapshot\nNAME                    USED  AVAIL  REFER  MOUNTPOINT\nzroot@nixos_base          0B      -    96K  -\nzroot/root@nixos_base    56K      -  5.98G  -\n</code></pre> <ol> <li> <p>On my TrueNAS system I'll do the same <code>zpool list</code>. We'll send the snapshot to <code>Pool0</code> <pre><code>admin@truenas[~]$ sudo zpool list\nNAME        SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT\nPool0      2.72T   277G  2.45T        -         -     0%     9%  1.00x    ONLINE  /mnt\nSSD0        111G  1.28M   111G        -         -     0%     0%  1.00x    ONLINE  /mnt\nboot-pool   102G  2.41G  99.6G        -         -     0%     2%  1.00x    ONLINE  -\n</code></pre></p> </li> <li> <p>We're not gonna send it over SSH like usual. We'll use Netcat (<code>nc</code>). This is fine for a local network but you should use other methods if you're sending it over the internet.</p> <p>On TrueNAS we'll run the server with the following command. <pre><code>nc -w 120 -l -p 8023 | sudo zfs receive Pool0/zfs_snapshots/nixos_$(date +\\%Y-\\%m-\\%d)\n</code></pre> 6. Now on our main machine we'll run the following. <code>sudo zfs send zroot/root@nixos_base | nc -w 20 192.168.1.9 8023</code></p> </li> <li> <p>After some time, the snapshot should be in the TrueNAS UI; But we can also view it from the command line.</p> </li> </ol> <pre><code>admin@truenas[~]$ sudo zfs list -t snapshot\nNAME                                              USED  AVAIL  REFER  MOUNTPOINT\nPool0@manual-2024-08-09_19-10                     133K      -   384K  -\nPool0@auto-2024-08-18_03-50                      95.9K      -   384K  -\nPool0@auto-2024-08-25_03-50                      95.9K      -   394K  -\nPool0@auto-2024-09-01_03-50                      95.9K      -   394K  -\nPool0/zfs_snapshots/nixos_2024-09-06@nixos_base   917M      -  9.73G  -\nSSD0@manual-2024-08-09_19-10                        0B      -    96K  -\nSSD0@auto-2024-08-18_03-50                          0B      -    96K  -\nSSD0@auto-2024-08-25_03-50                          0B      -    96K  -\nSSD0@auto-2024-09-01_03-50                          0B      -    96K  -\n</code></pre>"}]}