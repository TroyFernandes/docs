{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#introduction","title":"Introduction","text":"<p>I've been self-hosting &amp; homelabbing since 2016 and I often times find myself in this scenario:</p> <p></p><pre><code>&gt; Find something interesting on Github/Reddit\n&gt; Learn how to set it up in a test environment\n&gt; Move on &amp; forget about it\n&gt; Months later it shows up again\n&gt; I have some hardware that I can run this on permanently\n&gt; Forget how I set it up originally &amp; can't find the guides again\n&gt; ...\n</code></pre> <p></p> <p>I always prefer text guides over any other medium + it's always a bonus when they're short. </p> <p>So here's my version of that; very informal docs on things I've setup, to refer back to.</p> <p>Maybe you'll find it useful. </p>"},{"location":"ansible/simple_setup/","title":"Installing Ansible on Windows through WSL","text":""},{"location":"ansible/simple_setup/#installing-ansible-on-windows-through-wsl","title":"Installing Ansible on Windows through WSL","text":"<p>Requirements: WSL is already setup. Do all the steps below inside the WSL instance</p> <ol> <li> <p>Install Ansible &amp; sshpass: <code>pip install ansible</code> &amp; <code>sudo apt install sshpass</code></p> </li> <li> <p>Generate SSH Keys and Copy:     </p><pre><code>&gt; ssh-keygen -t rsa -b 4096 \n&gt; ssh-copy-id {username}@{remote_server_ip}\n</code></pre><p></p> </li> <li> <p>Create Playbook directory: <code>mkdir ansible_playbooks</code></p> </li> <li> <p>Create Inventory file with the following:</p> <p>inventory.yml </p><pre><code>[debian_lxc]\n192.168.1.173 ansible_ssh_user=root\n</code></pre> 5. Create playbook file with the follwing:<p></p> <p>test.yml </p><pre><code>---\n- name: Example Playbook\nhosts: debian_lxc\ntasks:\n- name: Hello World\n    debug:\n    msg: Hello, world!\n</code></pre><p></p> </li> <li> <p>Run the Playbook: <code>ansible-playbook -i inventory.ini test.yml</code></p> </li> </ol>"},{"location":"arch_linux/recover_systemd-boot/","title":"Recover systemd-boot","text":""},{"location":"arch_linux/recover_systemd-boot/#recover-systemd-boot","title":"Recover systemd-boot","text":"<p>Sometimes when updating your system (in my case using <code>yay</code>), when you go to reboot your boot manager you will only have the option <code>Reboot into firmware interface</code>.</p> <p>The issue can be anything really, but usually it happens when installing a package, and when an update comes around, something breaks.</p> <p>This guide was written with <code>EndeavourOS</code> in mind.</p>"},{"location":"arch_linux/recover_systemd-boot/#resources","title":"Resources","text":"<ul> <li>chroot</li> <li>arch-chroot</li> <li>Help recovering Systemd-boot</li> </ul>"},{"location":"arch_linux/recover_systemd-boot/#how-to","title":"How-to","text":"<ol> <li>Boot into the live environment; I used EndeavourOS.</li> <li>Change to root. <code>sudo su</code></li> <li>List the system partitions. <code>lsblk -f</code>. You should see something like this. The relevant section is <code>nvme0n1</code>, more specifically <code>nvme0n1p1</code> and <code>nvme0n1p2</code>. We need to mount both of these. <pre><code>[root@EndeavourOS liveuser]# lsblk -f\nNAME        FSTYPE   FSVER            LABEL       UUID                                 FSAVAIL FSUSE% MOUNTPOINTS\nloop0       squashfs 4.0                                                                     0   100% /run/archiso/airootfs\nsda\n\u251c\u2500sda1      exfat    1.0              Ventoy      4E21-0000\n\u2502 \u2514\u2500ventoy  iso9660  Joliet Extension EOS_202406  2024-06-25-11-30-26-00                     0   100% /run/archiso/bootmnt\n\u2514\u2500sda2      vfat     FAT16            VTOYEFI     223C-F3F8\nnvme0n1\n\u251c\u2500nvme0n1p1 vfat     FAT32                        5E74-E696\n\u251c\u2500nvme0n1p2 ext4     1.0              endeavouros 1a236633-49a9-4fa6-918a-8129dd30617a   33.5G    79% /run/media/liveuser/endeavouros\n\u2514\u2500nvme0n1p3 swap     1                swap        3556ab04-2191-4da9-b168-3df725fdaaab\n</code></pre></li> <li>Mount the root partition. <code>mount /dev/nvme0n1p2 /mnt</code></li> <li>Mount the efi partition. <code>mount /dev/nvme0n1p1 /mnt/efi</code></li> <li>arch-chroot into the mounted system. <code>arch-chroot /mnt</code></li> <li>Optional step. I need to uninstall the package which broke my system which in this case is <code>zfs-utils</code>. <code>pacman -Rns zfs-utils</code>. This removes the package as well as the dependencies.</li> <li>Now the actual fix. Reinstalling the kernel. Run <code>reinstall-kernels</code> The ouput should show no errors and look something like this: <pre><code>[root@EndeavourOS /]# reinstall-kernels\nInstalling kernel 6.10.8-arch1-1\ndracut[I]: Executing: /usr/bin/dracut --no-hostonly --force /efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd-fallback 6.10.8-arch1-1\ndracut[I]: *** Including module: systemd ***\ndracut[I]: *** Including module: systemd-ask-password ***\ndracut[I]: *** Including module: systemd-initrd ***\ndracut[I]: *** Including module: systemd-journald ***\ndracut[I]: *** Including module: systemd-modules-load ***\ndracut[I]: *** Including module: systemd-pcrphase ***\ndracut[I]: *** Including module: systemd-sysctl ***\ndracut[I]: *** Including module: systemd-sysusers ***\ndracut[I]: *** Including module: systemd-tmpfiles ***\ndracut[I]: *** Including module: systemd-udevd ***\ndracut[I]: *** Including module: modsign ***\ndracut[I]: *** Including module: i18n ***\ndracut[I]: *** Including module: btrfs ***\ndracut[I]: *** Including module: crypt ***\ndracut[I]: *** Including module: dm ***\ndracut[I]: *** Including module: kernel-modules ***\ndracut[I]: *** Including module: kernel-modules-extra ***\ndracut[I]: *** Including module: lvm ***\ndracut[I]: *** Including module: mdraid ***\ndracut[I]: *** Including module: nvdimm ***\ndracut[I]: *** Including module: pcmcia ***\ndracut[I]: *** Including module: qemu ***\ndracut[I]: *** Including module: qemu-net ***\ndracut[I]: *** Including module: systemd-cryptsetup ***\ndracut[I]: *** Including module: fido2 ***\ndracut[I]: *** Including module: pkcs11 ***\ndracut[I]: *** Including module: hwdb ***\ndracut[I]: *** Including module: lunmask ***\ndracut[I]: *** Including module: resume ***\ndracut[I]: *** Including module: rootfs-block ***\ndracut[I]: *** Including module: terminfo ***\ndracut[I]: *** Including module: udev-rules ***\ndracut[I]: *** Including module: virtiofs ***\ndracut[I]: *** Including module: dracut-systemd ***\ndracut[I]: *** Including module: usrmount ***\ndracut[I]: *** Including module: base ***\ndracut[I]: *** Including module: fs-lib ***\ndracut[I]: *** Including module: shutdown ***\ndracut[I]: *** Including modules done ***\ndracut[I]: *** Installing kernel module dependencies ***\ndracut[I]: *** Installing kernel module dependencies done ***\ndracut[I]: *** Resolving executable dependencies ***\ndracut[I]: *** Resolving executable dependencies done ***\ndracut[I]: *** Hardlinking files ***\ndracut[I]: *** Hardlinking files done ***\ndracut[I]: *** Generating early-microcode cpio image ***\ndracut[I]: *** Constructing GenuineIntel.bin ***\ndracut[I]: *** Store current command line parameters ***\ndracut[I]: *** Stripping files ***\ndracut[I]: *** Stripping files done ***\ndracut[I]: *** Creating image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd-fallback' ***\ndracut[I]: *** Creating initramfs image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd-fallback' done ***\ndracut[I]: Executing: /usr/bin/dracut --hostonly --no-hostonly-cmdline -f /efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd 6.10.8-arch1-1\ndracut[I]: *** Including module: systemd ***\ndracut[I]: *** Including module: systemd-ask-password ***\ndracut[I]: *** Including module: systemd-initrd ***\ndracut[I]: *** Including module: systemd-journald ***\ndracut[I]: *** Including module: systemd-modules-load ***\ndracut[I]: *** Including module: systemd-sysctl ***\ndracut[I]: *** Including module: systemd-sysusers ***\ndracut[I]: *** Including module: systemd-tmpfiles ***\ndracut[I]: *** Including module: systemd-udevd ***\ndracut[I]: *** Including module: i18n ***\ndracut[I]: *** Including module: kernel-modules ***\ndracut[I]: *** Including module: kernel-modules-extra ***\ndracut[I]: *** Including module: pcmcia ***\ndracut[I]: *** Including module: hwdb ***\ndracut[I]: *** Including module: resume ***\ndracut[I]: *** Including module: rootfs-block ***\ndracut[I]: *** Including module: terminfo ***\ndracut[I]: *** Including module: udev-rules ***\ndracut[I]: *** Including module: dracut-systemd ***\ndracut[I]: *** Including module: usrmount ***\ndracut[I]: *** Including module: base ***\ndracut[I]: *** Including module: fs-lib ***\ndracut[I]: *** Including module: shutdown ***\ndracut[I]: *** Including modules done ***\ndracut[I]: *** Installing kernel module dependencies ***\ndracut[I]: *** Installing kernel module dependencies done ***\ndracut[I]: *** Resolving executable dependencies ***\ndracut[I]: *** Resolving executable dependencies done ***\ndracut[I]: *** Hardlinking files ***\ndracut[I]: *** Hardlinking files done ***\ndracut[I]: *** Generating early-microcode cpio image ***\ndracut[I]: *** Constructing GenuineIntel.bin ***\ndracut[I]: *** Store current command line parameters ***\ndracut[I]: *** Stripping files ***\ndracut[I]: *** Stripping files done ***\ndracut[I]: *** Creating image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd' ***\ndracut[I]: *** Creating initramfs image file '/efi/343bef0b023b4aae84fd922fc9b99d13/6.10.8-arch1-1/initrd' done ***\n[root@EndeavourOS /]#\n</code></pre></li> </ol> <p>Reboot and you should see your regular boot manager options.</p>"},{"location":"docker/cheatsheet/","title":"Cheatsheet","text":""},{"location":"docker/cheatsheet/#cheatsheet","title":"Cheatsheet","text":""},{"location":"docker/cheatsheet/#rebuilding-a-dockerfile-image","title":"Rebuilding a Dockerfile &amp; Image","text":"<pre><code>docker compose build --no-cache\ndocker compose up -d\n</code></pre>"},{"location":"docker/cheatsheet/#view-live-docker-logs","title":"View Live docker logs","text":"<pre><code>docker logs --follow &lt;ID&gt;\n</code></pre>"},{"location":"docker/docker_context/","title":"Docker Context","text":""},{"location":"docker/docker_context/#docker-context","title":"Docker Context","text":"<p>How to manage another docker instance e.g. deploy/manage docker compose files on a Raspberry Pi</p>"},{"location":"docker/docker_context/#resources","title":"Resources","text":"<ul> <li>Docker Context</li> </ul>"},{"location":"docker/docker_context/#prerequisites","title":"Prerequisites","text":"<p>You should set up passwordless ssh access between your machines.</p>"},{"location":"docker/docker_context/#adding-a-remote-context","title":"Adding a Remote Context","text":"<ol> <li> <p>Create the context: <code>docker context create dietpi1 --docker \"host=ssh://root@192.168.1.11\"</code></p> </li> <li> <p>View the new context using: <code>docker context ls</code> </p><pre><code>/c/Windows/System32 \u276f docker context ls\nNAME              DESCRIPTION                               DOCKER ENDPOINT                             ERROR\ndefault           Current DOCKER_HOST based configuration   npipe:////./pipe/docker_engine\ndesktop-linux *   Docker Desktop                            npipe:////./pipe/dockerDesktopLinuxEngine\ndietpi1                                                     ssh://root@192.168.1.11\n</code></pre><p></p> </li> <li> <p>If you need to update the context: <code>docker context update --description \"RaspberryPi 3b; DietPi\" --docker \"host=ssh://root@192.168.1.11\" dietpi1</code></p> </li> </ol>"},{"location":"docker/docker_context/#deploying-a-container-to-the-remote","title":"Deploying a Container to the Remote","text":"<p>Simply create your docker compose file and run: <code>docker-compose --context dietpi1 up -d</code></p>"},{"location":"docker/healthchecks.io/","title":"Healthchecks.io","text":""},{"location":"docker/healthchecks.io/#healthchecksio","title":"Healthchecks.io","text":"<p>Setting up a Healthchecks container with e-mail notifications using Gmail.</p>"},{"location":"docker/healthchecks.io/#resources","title":"Resources","text":"<ul> <li>Healthchecks Docs</li> <li>Container Image</li> <li>Gmail App Passwords</li> </ul>"},{"location":"docker/healthchecks.io/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need an app password created for your gmail account you want to send emails from.</li> </ul>"},{"location":"docker/healthchecks.io/#setup","title":"Setup","text":"<ol> <li>Grab the docker-compose file from the container page. I'll show mine:</li> </ol> <pre><code>version: \"3\"\nservices:\n  healthchecks:\n    image: healthchecks/healthchecks:latest\n    container_name: healthchecks\n    environment:\n      - ALLOWED_HOSTS=*\n      - DB=sqlite\n      - DB_NAME=/data/hc.sqlite\n      - DEBUG=True\n      - DEFAULT_FROM_EMAIL=email@gmail.com\n      - EMAIL_HOST=smtp.gmail.com\n      - EMAIL_HOST_PASSWORD=qqqqwwwweeeerrrr\n      - EMAIL_HOST_USER=email@gmail.com\n      - EMAIL_PORT=587\n      - EMAIL_USE_TLS=True\n      - SECRET_KEY=---\n      - SITE_ROOT=http://192.168.1.11:8000\n    ports:\n      - 8000:8000\n    volumes:\n      - healthchecks-data:/data\n    restart: unless-stopped\nvolumes:\n    healthchecks-data:\n</code></pre> NOTE: When you create a gmail app password, it creates a 16 character password, and shows it to you like this: <code>qqqq wwww eeee rrrr</code>. You need to enter it like above with no spaces, otherwise it won't work and you'll get the error: <code>Server returned an error: 535-5.7.8 Username and Password not accepted</code> <ol> <li>Pull the image and once it's created, create the super-user account by running: </li> </ol> <p><code>docker-compose exec healthchecks /opt/healthchecks/manage.py createsuperuser</code></p>"},{"location":"docker/healthchecks.io/#creating-an-example-check","title":"Creating an Example Check","text":"<ol> <li> <p>There should be an example check created when you first setup healthchecks. You can also test out the email notifications under <code>Integrations</code>.</p> <p>What I find useful about healthchecks is that you can signal the start and end of a task you want to be monitored. My use-case is to monitor cron jobs; Let's see an example.</p> </li> <li> <p>We'll signal a start by appending <code>/start</code> to the url. To signal an end, simply omit the <code>/start</code> from the url. Here's an example cron job that rsyncs a directory from one drive to another.</p> </li> </ol> <pre><code>#!/bin/bash\ncurl -fsS -m 10 --retry 5 -o /dev/null http://192.168.1.11:8000/ping/f77a6fc5-ca6c-49fe-bfcd-9e5f2da335dd/start\n\nrsync --delete-after -avP /mnt/cache_ssd/Music/ /mnt/disk4/Music/\n\ncurl -fsS -m 10 --retry 5 -o /dev/null http://192.168.1.11:8000/ping/f77a6fc5-ca6c-49fe-bfcd-9e5f2da335dd\n</code></pre> <p>You'll need to configure the check with a proper grace period to get alerting. The healthchecks docs are easy to follow and show you how.</p>"},{"location":"homelab/network_diagram/","title":"Network diagram","text":""},{"location":"homelab/rack/","title":"Rack","text":""},{"location":"kubernetes/cheatsheet/","title":"Cheatsheet","text":""},{"location":"kubernetes/cheatsheet/#cheatsheet","title":"Cheatsheet","text":""},{"location":"kubernetes/cheatsheet/#get-all-running-namespaces","title":"Get all running namespaces","text":"<p><code>kubectl get svc --all-namespaces -o wide</code></p>"},{"location":"kubernetes/cheatsheet/#create-a-deployment","title":"Create a deployment","text":"<p><code>kubectl create -f [filename.yml]</code></p>"},{"location":"kubernetes/cheatsheet/#delete-a-deployment","title":"Delete a deployment","text":"<p><code>kubectl delete -f [filename.yml]</code></p>"},{"location":"kubernetes/cheatsheet/#stop-pod-by-namespace","title":"Stop pod by namespace","text":"<p><code>kubectl delete --all pods --namespace=[namespace]</code></p>"},{"location":"kubernetes/cheatsheet/#stop-deployment-by-namespace","title":"Stop deployment by namespace","text":"<p><code>kubectl delete --all deployments --namespace=firefox</code></p>"},{"location":"kubernetes/cheatsheet/#switch-to-namespace","title":"Switch to namespace","text":"<p><code>kubectl config set-context --current --namespace=my-namespace</code></p>"},{"location":"kubernetes/cheatsheet/#edit-deployment","title":"Edit Deployment","text":"<p><code>kubectl edit deployment/my-nginx</code></p>"},{"location":"kubernetes/cheatsheet/#restart-deployment","title":"Restart Deployment","text":"<p><code>kubectl rollout restart deployment &lt;deployment_name&gt; -n &lt;namespace&gt;</code></p>"},{"location":"kubernetes/cheatsheet/#delete-service","title":"Delete Service","text":"<p><code>kubectl delete service kubernetes-dashboard -n kube-system</code></p>"},{"location":"kubernetes/cheatsheet/#delete-namespace","title":"Delete namespace","text":"<p><code>kubectl delete ns cattle-system</code></p>"},{"location":"misc/locking_down_vps/","title":"Setting up &amp; Locking Down a New VPS","text":""},{"location":"misc/locking_down_vps/#setting-up-locking-down-a-new-vps","title":"Setting up &amp; Locking Down a New VPS","text":"<p>First things to do when setting up a new VPS. For this example, I'm using a VPS running Debian 13 (Trixie).</p>"},{"location":"misc/locking_down_vps/#resources","title":"Resources","text":"<ul> <li>Syntax Youtube - Selfhost 101</li> </ul>"},{"location":"misc/locking_down_vps/#how-to","title":"How To","text":"<ol> <li> <p>Run updates with <code>apt update</code> &amp; <code>apt upgrade</code></p> </li> <li> <p>Install sudo if not installed with <code>apt install sudo</code></p> </li> <li> <p>Change root password with <code>passwd</code></p> </li> <li> <p>Add a new user with <code>adduser {username}</code></p> </li> <li> <p>Add the new user to sudo'ers group: <code>adduser {username} sudo</code> or <code>usermod -aG sudo {username}</code></p> </li> <li> <p>Setup key based SSH logins (Do the following on your personal PC)</p> <ol> <li>Generate SSH Keys if you haven't already: <code>ssh-keygen -t ed25519</code></li> <li>Copy the keys over to the VPS: <code>ssh-copy-id {username}@{VPS-ip-address}</code></li> </ol> </li> <li> <p>Disable password login &amp; root login.</p> <pre><code>&gt; sudo nano /etc/ssh/sshd_config\n\nChange \"PasswordAuthentication\" from \"yes\" to \"no\"\nChange \"PermitRootLogin\" to \"no\"\n\n&gt; sudo service ssh restart\n</code></pre> </li> <li> <p>Install unattended-upgrades:</p> <pre><code>&gt; sudo apt install unattended-upgrades\n&gt; sudo dpkg-reconfigure unattended-upgrades\n</code></pre> </li> </ol>"},{"location":"opnsense/caddy/","title":"Caddy Reverse Proxy with Lets Encrypt","text":""},{"location":"opnsense/caddy/#caddy-reverse-proxy-with-lets-encrypt","title":"Caddy Reverse Proxy with Lets Encrypt","text":"<p>It's time to finally get rid of those browser warnings when hosting services using <code>HTTP</code>. We'll setup caddy as an internal reverse proxy and use DNS-01 Challenges to get a valid certificates from Let's Encrypt. </p> <p>Nothing needs to, or will be exposed from outside our network to achieve this.</p> <p>We'll use our domain providers' <code>(Porkbun)</code> to API generate and grab the certs.</p> <p>This will also enable us to reach our services with friendly names. Instead of going to Gitea with <code>192.168.1.10:3000</code>, we'll do something like <code>gitea.example.ca</code></p>"},{"location":"opnsense/caddy/#resources","title":"Resources","text":"<ul> <li>Caddy</li> <li>OPNsense Caddy Guide</li> <li>Porkbun</li> <li>Home Network Guy Unbound DNS Overrides</li> </ul>"},{"location":"opnsense/caddy/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>AdguardHome working properly with Unbound (Private reverse dns resolvers)</li> <li> <p>A domain you own; I have one registered with Porkbun. We'll assume I bought the domain <code>example.ca</code></p> </li> <li> <p>An API Key &amp; Secret Key generated for your domain through Porkbun.</p> </li> </ul>"},{"location":"opnsense/caddy/#what-is-what","title":"What is What","text":"<p>I use OPNSense's default DNS resolver <code>Unbound</code> in conjunction with <code>AdguardHome</code> for ad-blocking</p> <ul> <li>Unbound is running on OPNsense with the ip: <code>192.168.1.1:5353</code></li> <li>AdguardHome is running on OPNsense with the ip: <code>192.168.1.1:53</code></li> <li>Caddy will run on OPNsense with the ip: <code>192.168.1.1:80</code> and <code>192.168.1.1:443</code></li> </ul>"},{"location":"opnsense/caddy/#how-to","title":"How To","text":""},{"location":"opnsense/caddy/#unbound","title":"Unbound","text":"<p>Follow the guide from Home Network Guy to create the override.</p> <p>Make sure the IP address is the IP of OPNsense; In my case it is <code>192.168.1.1</code>.</p> <p>The only change when following the guide is to use a wildcard instead of the service name. In his guide, he entered <code>reverse-proxy</code> as the host, but we instead will do <code>*</code> as the host.</p> <p>Don't create the alias.</p> <p></p>"},{"location":"opnsense/caddy/#adguardhome","title":"AdguardHome","text":"<p>Since this is internal, we'll need to do a DNS rewrite to tell our DNS server to resolve any requests to <code>*.example.ca</code>.</p> <p>Under <code>Filters -&gt; DNS Rewrites</code> Add an entry with <code>*.example.ca</code> as the domain and <code>192.168.1.1</code> as the IP.</p> <p>This will let AdguardHome let Unbound resolve these requests.</p> <p></p>"},{"location":"opnsense/caddy/#caddy","title":"Caddy","text":"<ol> <li> <p>Start from the installation but skip creating the WAN firewall rule.</p> </li> <li> <p>Then continue following Creating a Simple Reverse Proxy but before doing Add Domain go to DNS Provider and select Porkbun as the provider.</p> <p>Enter your API Key from porkbun into the DNS API Standard Field and under DNS API Additional Field 1 add your Secret Key from porkbun.</p> </li> <li> <p>Continue following the guide to add a domain, but make sure to check the DNS-01 Challenge box when adding your domain.</p> </li> <li> <p>Stop before HTTP Handlers. We'll do something before this.</p> </li> <li> <p>Since we're using wildcards, we'll create subdomains for each of our services. Create a new Sub-Domain, and make sure the domain you created before is selected. Name the subdomain whatever you want. e.g if you use a domain called <code>*.example.ca</code> then your subdomain would be <code>gitea.example.ca</code></p> </li> </ol> <p></p> <ol> <li>Continue creating the HTTP Handler, but make sure to select the sub-domain we created in step 5.</li> </ol> <p></p> <ol> <li>That should be everything setup now. Caddy should grab the certs and you should now be able to go to <code>gitea.example.ca</code> and see your gitea instance being served with <code>https</code>.</li> </ol>"},{"location":"opnsense/network_booting/","title":"Network Booting with netboot.xyz &amp; OPNsense","text":""},{"location":"opnsense/network_booting/#network-booting-with-netbootxyz-opnsense","title":"Network Booting with netboot.xyz &amp; OPNsense","text":"<p>Network booting is useful when you want to image a machine without having to carry around a USB drive. My main use case for setting this up is to eventually netboot raspberry pi's and mount their boot drives over NFS. </p>"},{"location":"opnsense/network_booting/#resources","title":"Resources","text":"<ul> <li>netboot.xyz</li> <li>General steps to setting up netbooting on OPNsense by u/Asche77</li> <li>In-depth guide on netbooting with OPNsense</li> <li>Complete netbootxyz/unifi/proxmox guide by TechnoTim</li> </ul> <p>Related</p> <ul> <li>Troubleshooting netbooting within a Proxmox VM</li> </ul>"},{"location":"opnsense/network_booting/#what-is-what","title":"What is what","text":"<ul> <li>OPNsense IP is <code>192.168.1.1</code></li> <li>TFTP server will be <code>192.168.1.1</code></li> <li>DHCP server is <code>192.168.1.1</code></li> <li>netbootxyz server is <code>192.168.1.11</code></li> </ul>"},{"location":"opnsense/network_booting/#how-to","title":"How To","text":""},{"location":"opnsense/network_booting/#netbootxyz-docker-setup","title":"netboot.xyz Docker Setup","text":"<ol> <li>Setup is pretty easy, just choose your install method. I'll use a docker compose file to install the container on a Debian LXC Container.</li> </ol> <pre><code>version: \"2.1\"\nservices:\n  netbootxyz:\n    image: ghcr.io/netbootxyz/netbootxyz\n    container_name: netbootxyz\n    volumes:\n      - ./config:/config\n      - ./assets:/assets\n    ports:\n      - 3000:3000\n      - 69:69/udp\n      - 8080:80\n    restart: unless-stopped\n</code></pre>"},{"location":"opnsense/network_booting/#tftp-server-opnsense-settings","title":"TFTP Server &amp; OPNsense Settings","text":"<ol> <li> <p>Install the TFTP server on OPNsense by going to [System] -&gt; [Plugin] -&gt; search for <code>tft</code>, and install <code>os-tftp</code></p> </li> <li> <p>The server won't start unless the <code>/usr/local/tftp</code> directory exists, so create it.</p> </li> <li> <p>Download the netboot.xyz efi files from the releases page and place them in the directory you created. You can use the fetch command in freebsd like <code>fetch https://github.com/netbootxyz/netboot.xyz/releases/download/2.0.82/netboot.xyz.efi</code></p> <p>I'll download the following:</p> <ul> <li>netboot.xyz-arm64.efi</li> <li>netboot.xyz.efi</li> <li>netboot.xyz.kpxe</li> </ul> </li> <li> <p>Add your netboot server to your DHCP server by going to [Services] -&gt; [ISC DHCPv4] -&gt; [Interface] -&gt; [Network booting].</p> </li> <li> <p>Add your netboot server under [Services] -&gt; [ISC DHCPv4] -&gt; [Interface] -&gt; [TFTP server].</p> </li> </ol> <p></p> <ol> <li>You should be able to now boot from the network</li> </ol>"},{"location":"opnsense/pppoe/","title":"Switching ISP to PPPoE w/ IPv4 &amp; IPv6","text":""},{"location":"opnsense/pppoe/#switching-isp-to-pppoe-w-ipv4-ipv6","title":"Switching ISP to PPPoE w/ IPv4 &amp; IPv6","text":"<p>Finally escaping the clutches of Rogers and switching to E-BOX (Bell Subsidiary \ud83d\ude14). The ONLY thing I'll miss from Rogers is the static IPv4 address they provide. So I'll need to re-configure my Wireguard server, but that'll be for another guide.</p>"},{"location":"opnsense/pppoe/#resources","title":"Resources","text":"<ul> <li>OPNSense PPPoE Setup</li> <li>Blog about IPv6 &amp; E-BOX</li> <li>E-BOX FAQ PPPoE Credentials</li> </ul>"},{"location":"opnsense/pppoe/#pre-requisites","title":"Pre-requisites","text":"<p>Just grab your PPPoE credentials from your E-BOX Customer Zone and note down the VLAN ID (In this case it's 40)</p>"},{"location":"opnsense/pppoe/#how-to","title":"How To","text":""},{"location":"opnsense/pppoe/#pppoe-setup","title":"PPPoE Setup","text":"<p>This is actually extremely easy. Just follow the OPNsense guide exactly. There's nothing specific you need to set in regards to EBOX other than the vlan tag 40</p> <p>After you plug in the cable from the ONT to your OPNsense box, you should have an internet connection with an IPv4 address.</p>"},{"location":"opnsense/pppoe/#setting-up-ipv6-support","title":"Setting up IPv6 Support","text":"<p>NOTE: This is just to get an IPv6 address. I still haven't setup IPv6 on my LAN side permanently other than for testing.</p> <p>The blogpost above has all the information, but the guide is for pfSense. I'll show you where the settings are in OPNSense. We'll only be doing the first few paragraphs of the guide.</p> <ol> <li>Go to the interface setup you created for PPPoE under(In my case I named it EBOX_PPPoE) under Interfaces -&gt; [EBOX_PPPoE]</li> <li>Change IPv6 Configuration Type to: DHCPv6</li> <li>Under DHCPv6 client configuration change the following<ul> <li>Prefix delegation size: 56</li> <li>Request prefix only: Checked</li> <li>Send prefix hint: Checked</li> </ul> </li> <li>Reboot your opnsense box (maybe you dont need to do this) and you should recieve an IPv6 address.</li> </ol>"},{"location":"opnsense/pppoe/#e-box-thoughts","title":"E-BOX Thoughts","text":"<p>I've been using EBOX for about 2 weeks now, and it's been great so far. My OPNsense has an Intel G4600 and has no issue running a symmetric 1 gig link.</p> <p>I know about the issue of EBOX routing all traffic through Montreal, and doing so results in a RTT in OPNsense around 12ms from Toronto. This is still lower than Rogers. I've seen no issues in online games. CS2 &amp; Overwatch is 10ms lower than Rogers at around 30ms. TFT is around 5ms lower at around 25-30ms. Wish I got a bigger improvement, but as long as it's not worse I'm happy. </p> <p>I've noticed however, the IPv4 address they give shows a proper location of Toronto, but the IPv6 address is Montreal. I haven't done any further testing on this.</p> <p>The IPv4 address they give isn't static, but the IPv6 one is static.</p>"},{"location":"software_dev_tools/git/cheatsheet/","title":"Cheatsheet","text":""},{"location":"software_dev_tools/git/cheatsheet/#cheatsheet","title":"Cheatsheet","text":""},{"location":"software_dev_tools/git/cheatsheet/#ignore-local-changes-delete-new-files-and-pull-from-remote-my-favourite","title":"Ignore local changes, delete new files and pull from remote <sub><sub><sub>(my favourite)</sub></sub></sub>","text":"<pre><code>git reset --hard\ngit clean -fxd\ngit pull\n</code></pre>"},{"location":"software_dev_tools/gitea/gitea-actions/","title":"Setup Gitea Actions","text":""},{"location":"software_dev_tools/gitea/gitea-actions/#setup-gitea-actions","title":"Setup Gitea Actions","text":"<p>Guide to setting up Gitea Actions under docker</p>"},{"location":"software_dev_tools/gitea/gitea-actions/#why","title":"Why?","text":"<ol> <li>Replicate Github Actions locally</li> <li>It's annoying to verify Github Actions by making a push to a repo. Gitea actions are supposed to be a drop in replacement for Github Actions.</li> <li>You can assign way more hardware to the runner</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-actions/#goals-at-the-end-of-the-day","title":"Goals at the end of the day","text":"<p>We should have runners setup and available to execute the actions within our repos.</p>"},{"location":"software_dev_tools/gitea/gitea-actions/#resources","title":"Resources","text":"<ul> <li>Gitea Actions</li> <li>Gitea Actions Quickstart</li> <li>Gitea Act Runner</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-actions/#requirements","title":"Requirements","text":"<ol> <li>A gitea server</li> <li>A machine with docker installed as well as docker-compose</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-actions/#how-to","title":"How-To","text":""},{"location":"software_dev_tools/gitea/gitea-actions/#installing-act-runner","title":"Installing Act Runner","text":"<ol> <li> <p>We need to get the registration token from our gitea server. Within the gitea server, go into the terminal and run: <code>gitea --config /etc/gitea/app.ini actions generate-runner-token</code>. We'll get a token looking similar to this: <code>4ALVqHi8lx8Yk34RIJCn9pyqgafFDjEMpNQxHEpi</code></p> <ul> <li>Note: You can't run the above command as root. Make sure you're logged into an account that isnt root. e.g: <code>su - gitea</code></li> </ul> </li> <li> <p>Create the <code>docker-compose.yml</code> file. The Gitea docs have a standard template, I'll show you what I used: </p><pre><code>version: \"3.8\"\nservices:\n  runner:\n    image: gitea/act_runner:nightly\n    environment:\n      CONFIG_FILE: /config.yaml\n      GITEA_INSTANCE_URL: \"http://192.168.1.232:3000/\"\n      GITEA_RUNNER_REGISTRATION_TOKEN: \"4ALVqHi8lx8Yk34RIJCn9pyqgafFDjEMpNQxHEpi\"\n      # GITEA_RUNNER_NAME: \"${RUNNER_NAME}\"\n      # GITEA_RUNNER_LABELS: \"${RUNNER_LABELS}\"\n    volumes:\n      - ./config/config.yaml:/config.yaml\n      - ./data:/data\n      - /var/run/docker.sock:/var/run/docker.sock\n</code></pre><p></p> </li> <li> <p><code>docker-compose up -d</code> and the runner should register with your gitea instance.</p> </li> <li> <p>Go to your gitea dashboard and do: <code>Settings -&gt; Actions -&gt; Runners</code> and you should see the runner you just created.</p> </li> <li> <p>The runner is setup now. Lets clone a repo and add an action.</p> </li> <li> <p>To add an action, within the root dir, create directory <code>.gitea/workflows/</code></p> </li> <li> <p>Add your action file. Name it whatever you like with the extension <code>.yml</code>. Lets use the example from Gitea.</p> </li> </ol> <pre><code>name: Gitea Actions Demo\nrun-name: ${{ gitea.actor }} is testing out Gitea Actions\non: [push]\n\njobs:\n  Explore-Gitea-Actions:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"The job was automatically triggered by a ${{ gitea.event_name }} event.\"\n      - run: echo \"This job is now running on a ${{ runner.os }} server hosted by Gitea!\"\n      - run: echo \"The name of your branch is ${{ gitea.ref }} and your repository is ${{ gitea.repository }}.\"\n      - name: Check out repository code\n        uses: actions/checkout@v4\n      - run: echo \"The ${{ gitea.repository }} repository has been cloned to the runner.\"\n      - run: echo \"The workflow is now ready to test your code on the runner.\"\n      - name: List files in the repository\n        run: |\n          ls ${{ gitea.workspace }}\n      - run: echo \"This job's status is ${{ job.status }}.\"\n</code></pre> <ol> <li>Now push the repo. You should see a little yellow dot next to your username, above the topmost folder on the Gitea dashboard. You can click into it to check the status of the action.</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-actions/#end","title":"End","text":"<p>Our Gitea server should now have runners available to execute actions within our repos.</p>"},{"location":"software_dev_tools/gitea/gitea-actions/#final-thoughts-learnings","title":"Final Thoughts / Learnings","text":"<p>My initial assumption was that gitea had the capability to execute actions out of the box, but this isn't the case and the reason makes sense.</p> <p>It's better to have \"Ephemeral\" runners. Which esentilally means they're created when needed, then destroyed after. This allows you to not worry about having a cleanup script after the action has completed, ensuring the action is always run in a repetable, clean state.</p> <p>You'll notice within the docker compose file we bind the docker sock. This allows the runner to create seperate containers that are destroyed after the action has completed.</p>"},{"location":"software_dev_tools/gitea/gitea-server/","title":"Self-Hosted Gitea Server","text":""},{"location":"software_dev_tools/gitea/gitea-server/#self-hosted-gitea-server","title":"Self-Hosted Gitea Server","text":"<p>Guide to setting up a self-hosted Gitea server in an LXC containter on Proxmox</p>"},{"location":"software_dev_tools/gitea/gitea-server/#why","title":"Why?","text":"<ul> <li>Having a private git repo with a frontend for projects seperate from Github</li> <li>Run the equivalent of Github Actions but locally</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-server/#goals-at-the-end-of-the-day","title":"Goals at the end of the day","text":"<ul> <li>Have a local private Gitea Server running, accessible on the local network</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-server/#resources","title":"Resources","text":"<ul> <li>Gitea</li> <li>Proxmox Helper Scripts - Gitea</li> <li>Proxmox Helper Scripts - PostgreSQL</li> <li>PostgreSQL Post-Install</li> <li>Permission Denied fix when initial Gitea setup</li> </ul>"},{"location":"software_dev_tools/gitea/gitea-server/#requirements","title":"Requirements","text":"<p>The Gitea LXC container doesn't have a database, so we need to set one up. We will use a PostgreSQL LXC container.</p>"},{"location":"software_dev_tools/gitea/gitea-server/#how-to","title":"How-To","text":""},{"location":"software_dev_tools/gitea/gitea-server/#installing-postgresql","title":"Installing PostgreSQL","text":"<ol> <li> <p>Run the Proxmox Helper Script; Follow the prompts from the script and install Adminer</p> </li> <li> <p>Follow the post-install guide up until creating the new database.</p> </li> <li> <p>Create a new user named <code>gitea</code></p> <ul> <li><code>CREATE USER gitea WITH PASSWORD 'your-password';</code></li> </ul> </li> <li> <p>Create a new database named <code>gitea</code></p> <ul> <li><code>CREATE DATABASE gitea;</code></li> </ul> </li> <li> <p>Grant the user <code>gitea</code> access to the database <code>gitea</code></p> <ul> <li><code>GRANT ALL ON DATABASE gitea TO gitea;</code></li> </ul> </li> <li> <p>Change the owner of the <code>gitea</code> database</p> <ul> <li><code>ALTER DATABASE gitea OWNER TO gitea;</code></li> </ul> </li> </ol>"},{"location":"software_dev_tools/gitea/gitea-server/#installing-gitea","title":"Installing Gitea","text":"<ol> <li>Run the Proxmox Helper Script</li> </ol>"},{"location":"software_dev_tools/gitea/gitea-server/#end","title":"End","text":"<p>We should now have a running gitea server with a postgreSQL database</p>"},{"location":"zfs/snapshots/","title":"Snapshots","text":""},{"location":"zfs/snapshots/#snapshots","title":"Snapshots","text":"<p>This will be a guide on how to leverage the features of ZFS and create snapshots of machines running ZFS.</p> <p>We'll then send them to another machine also running ZFS to have a backup.</p>"},{"location":"zfs/snapshots/#resources","title":"Resources","text":"<ul> <li>ZFS Snapshot Briefer</li> <li>Fast ZFS Send with Netcat</li> </ul>"},{"location":"zfs/snapshots/#requirements","title":"Requirements","text":"<ul> <li>Two machines configured with ZFS</li> <li>If you want to browse the snapshots via SMB/NFS you'll need to create the share on TrueNAS. I made one called <code>zfs_snapshots</code></li> </ul> <p>I'll be using my laptop running <code>NixOS 24.05</code> with ZFS as the machine to backup. The backup server(target) will be a machine running <code>TrueNAS SCALE Dragonfish-24.04.2</code></p>"},{"location":"zfs/snapshots/#how-to","title":"How-To","text":"<ol> <li>View your zfs pools using <code>zpool list</code></li> </ol> <pre><code>[troy@nixos:~]$ zpool list\nNAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT\nzroot   228G  5.99G   222G        -         -     0%     2%  1.00x    ONLINE  -\n</code></pre> <ol> <li>We'll create a snapshot of the zroot dataset. <code>sudo zfs snapshot -r DATASET@NAME</code>. Note the <code>-r</code> flag. This will create a snapshot of the dataset and all of its children. We'll run this initially and every subsequent snapshot you can exlude this flag.</li> </ol> <pre><code>[troy@nixos:~]$ sudo zfs snapshot -r zroot@nixos_base\n</code></pre> <ol> <li>You can view the snapshots using <code>zfs list -t snapshot</code></li> </ol> <pre><code>[troy@nixos:~]$ zfs list -t snapshot\nNAME                    USED  AVAIL  REFER  MOUNTPOINT\nzroot@nixos_base          0B      -    96K  -\nzroot/root@nixos_base    56K      -  5.98G  -\n</code></pre> <ol> <li> <p>On my TrueNAS system I'll do the same <code>zpool list</code>. We'll send the snapshot to <code>Pool0</code> </p><pre><code>admin@truenas[~]$ sudo zpool list\nNAME        SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT\nPool0      2.72T   277G  2.45T        -         -     0%     9%  1.00x    ONLINE  /mnt\nSSD0        111G  1.28M   111G        -         -     0%     0%  1.00x    ONLINE  /mnt\nboot-pool   102G  2.41G  99.6G        -         -     0%     2%  1.00x    ONLINE  -\n</code></pre><p></p> </li> <li> <p>We're not gonna send it over SSH like usual. We'll use Netcat (<code>nc</code>). This is fine for a local network but you should use other methods if you're sending it over the internet.</p> <p>On TrueNAS we'll run the server with the following command. </p><pre><code>nc -w 120 -l -p 8023 | sudo zfs receive Pool0/zfs_snapshots/nixos_$(date +\\%Y-\\%m-\\%d)\n</code></pre> 6. Now on our main machine we'll run the following. <code>sudo zfs send zroot/root@nixos_base | nc -w 20 192.168.1.9 8023</code><p></p> </li> <li> <p>After some time, the snapshot should be in the TrueNAS UI; But we can also view it from the command line.</p> </li> </ol> <pre><code>admin@truenas[~]$ sudo zfs list -t snapshot\nNAME                                              USED  AVAIL  REFER  MOUNTPOINT\nPool0@manual-2024-08-09_19-10                     133K      -   384K  -\nPool0@auto-2024-08-18_03-50                      95.9K      -   384K  -\nPool0@auto-2024-08-25_03-50                      95.9K      -   394K  -\nPool0@auto-2024-09-01_03-50                      95.9K      -   394K  -\nPool0/zfs_snapshots/nixos_2024-09-06@nixos_base   917M      -  9.73G  -\nSSD0@manual-2024-08-09_19-10                        0B      -    96K  -\nSSD0@auto-2024-08-18_03-50                          0B      -    96K  -\nSSD0@auto-2024-08-25_03-50                          0B      -    96K  -\nSSD0@auto-2024-09-01_03-50                          0B      -    96K  -\n</code></pre>"}]}